{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required import\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import docx2txt\n",
    "import spacy\n",
    "import re\n",
    "from docx import Document\n",
    "\n",
    "from docx.shared import RGBColor\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "\n",
    "import docx\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words in Parties \n",
    "\n",
    "Parties = [\"Unilateral\", \"One-Way\", \"One Way\", \"company (\\\"Disclosing Party\\\")\", \"company (\\\"Discloser\\\")\", \"corporation (\\\"Disclosing Party\\\")\", \"corporation (\\\"Discloser\\\")\",  \"LLC (\\\"Disclosing Party\\\")\", \"LLC (\\\"Discloser\\\")\",  \"Inc. (\\\"Disclosing Party\\\")\", \"Inc. (\\\"Discloser\\\")\",  \"Incorporated (\\\"Disclosing Party\\\")\", \"Incorporated (\\\"Discloser\\\")\", \"Co. (\\\"Disclosing Party\\\")\", \"Co. (\\\"Discloser\\\")\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words in Residuals/ Memories\n",
    "\n",
    "Residuals = [\"residual\", \"residuals\", \"memories\", \"unaided\", \"memory\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arealdy existed words in Limitation of Liability\n",
    "\n",
    "Limitation = [\"Limitation of Liability\", \"under no circumstances\", \"shall be limited\", \"special, incidental, indirect or consequential damages\", \"punitive\",\"exemplary\",\"consequential\",\"indirect\",\"incidental\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words in Non-competition\n",
    "\n",
    "Noncompetition = [\"compete\", \"competition\", \"non-compete\", \"non-competition\", \"non compete\", \"non competition\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words in Non-solicitation\n",
    "\n",
    "Nonsolicitation = [\"non-solicitation\", \"solicit\", \"non-solicit\", \"non-servicing\",   \"nonsolicitation\", \"nonsolicit\", \"nonservicing\",  \"solicit\",  \"non solicitation\", \"non solicit\", \"non servicing\",  \"no solicit\", \"no-solicit\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words in Indemnification\n",
    "\n",
    "Indemnification = [\"indemnification\", \"indemnity\", \"hold-harmless\", \"hold harmless\", \"indemnify\", \"indemnified\", \"indemnifying\", \"defend\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words in Governing Law/ Jurisdiction\n",
    "\n",
    "Governing = [\"Texas\", \"Italy\", \"Italian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words in Exceptions\n",
    "\n",
    "Exceptions =  [\"was communicated by Disclosing Party to an unaffiliated third party free of any obligation of confidence\",   \"was communicated by Disclosing Party to a third party free of any obligation of confidence\", \"was communicated by Disclosing Party to  a third party without an obligation of confidence\", \"was disclosed by Disclosing Party to  a third party without an obligation of confidence\", \" was disclosed by Discloser to a third party without an obligation of confidentiality\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words for Representatives\n",
    "\n",
    "# Different from the above, this part is for existence, if exist, yes, if not exist, output a warning sign about \"Missing Representatives\"\n",
    "\n",
    "Representatives = [\"responsible\", \"liable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words for Remedies\n",
    "\n",
    "# Same as the last one, this part is for existence, if exist, yes, if not exist, output a warning sign about \"Missing Remedies\"\n",
    "Remedies = [\"injunction\", \"injunctive\",\"equitable\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words for Privacy / Personal Information\n",
    "Privacy = [\"Personal Data\", \"Personal Information\",\"PII\", \"GDPR\", \"CCPA\", \"CPRA\", \"Privacy\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Trial<br>\n",
    "输入路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##           ###           ##  EEEEEEEEEEEEEEEE      III\n",
    " ##         ## ##         ##   E                     III\n",
    "  ##       ##   ##       ##    E                     III\n",
    "   ##     ##     ##     ##     EEEEEEEEEEEEEEE       III\n",
    "    ##   ##       ##   ##      E                     III\n",
    "     ## ##         ## ##       E                     III\n",
    "      ##            ##         EEEEEEEEEEEEEEEE      III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2\n",
    "# 改一下你的输入路径 注意r \\\\ 和 docx别动\n",
    "doc = docx.Document(r'C:\\\\Users\\\\dell-pc\\\\Desktop\\\\NDA.docx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = ''\n",
    "\n",
    "for para in doc.paragraphs:\n",
    "    paragraphs += para.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment paragraph into sentences. \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# take unicode string  \n",
    "sentences = nlp(paragraphs)\n",
    "\n",
    "sentencess = list(sentences.sents)\n",
    "\n",
    "separated_sentences = []\n",
    "\n",
    "# list all sentences\n",
    "for i in range(len(sentencess)):\n",
    "    sentence = sentencess[i].text\n",
    "    \n",
    "    #sentence_separation = re.split('\\t|\\n|\\n\\t', sentence)\n",
    "    \n",
    "    sentence = sentence.replace('\\n\\n', ' ')\n",
    "    sentences = re.split(r'\\.(?=\\s|$)', sentence)\n",
    "\n",
    "\n",
    "    separated_sentences += sentences\n",
    "\n",
    "final_separated_sentences = [x for x in separated_sentences if x != '']\n",
    "\n",
    "## final_separated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall in previous section, we have already find synonyms of dealbreaker words and list them. \n",
    "\n",
    "## Parties\n",
    "## Residuals\n",
    "## Limitation\n",
    "## Noncompetition\n",
    "## Nonsolicitation\n",
    "## Indemnification\n",
    "## Representatives\n",
    "## Remedies\n",
    "## Privacy\n",
    "## Governing\n",
    "## Exceptions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary for dealbreaker catergories and words with each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of sensitive words, grouped them by different categories. \n",
    "\n",
    "dealbreaker_words = {\n",
    "    \"Parties\":Parties,\n",
    "     \"Residuals/Memories\": Residuals,\n",
    "     \"Limitation of Liability\": Limitation,\n",
    "     \"Non-competition\": Noncompetition,\n",
    "     \"Non-solicitation\":Nonsolicitation,\n",
    "     \"Indemnification\": Indemnification,\n",
    "     \"Governing Law/Jurisdiction\":Governing,\n",
    "     \"Exceptions\":Exceptions,\n",
    "     \"Privacy\": Privacy\n",
    "}\n",
    "\n",
    "# The reason why not including remedies and representatives is due to the reason that\n",
    "# their search methods are for existence (recall if they exist, then everything is ok, but if not exist, Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_list = []\n",
    "for i in range(len(final_separated_sentences)):\n",
    "    sentence = final_separated_sentences[i]\n",
    "    # Iterate through the sensitive words grouped by category\n",
    "    for category, words in dealbreaker_words.items():\n",
    "        # Use regular expressions to search for the words in the sentence\n",
    "        for word in words:\n",
    "            matches = re.finditer(r'\\b'+word+r'\\b', sentence, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                check_list.append(match.group())\n",
    "\n",
    "## check_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean it for unique words, Upper or lower case matters!\n",
    "\n",
    "check_list = list(set(check_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representatives\n",
    "representative_warning = True\n",
    "\n",
    "for i in range(len(separated_sentences)):\n",
    "    sentence = separated_sentences[i]\n",
    "    for word in Representatives:\n",
    "        if word in sentence:\n",
    "            representative_warning = False\n",
    "            ## print(f\"Found '{word}' in sentence: {sentence}\")\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remedies\n",
    "remedies_warning = True\n",
    "\n",
    "for i in range(len(separated_sentences)):\n",
    "    sentence = separated_sentences[i]\n",
    "    for word in Remedies:\n",
    "        if word in sentence:\n",
    "            remedies_warning = False\n",
    "            ## print(f\"Found '{word}' in sentence: {sentence}\")\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##           ###           ##  EEEEEEEEEEEEEEEE      III\n",
    " ##         ## ##         ##   E                     III\n",
    "  ##       ##   ##       ##    E                     III\n",
    "   ##     ##     ##     ##     EEEEEEEEEEEEEEE       III\n",
    "    ##   ##       ##   ##      E                     III\n",
    "     ## ##         ## ##       E                     III\n",
    "      ##            ##         EEEEEEEEEEEEEEEE      III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import document again!\n",
    "# 改你的路径哦，不要动r \\\\ 和docx\n",
    "doc=docx.Document(r'C:\\\\Users\\\\dell-pc\\\\Desktop\\\\NDA.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key_by_value(value, dictionary):\n",
    "    keys = []\n",
    "    for key, val in dictionary.items():\n",
    "        if isinstance(val, list):\n",
    "            if any(v.lower() == value.lower() for v in val):\n",
    "                keys.append(key)\n",
    "        elif val.lower() == value.lower():\n",
    "            keys.append(key)\n",
    "    return keys if keys else None  # if value is not found in any key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving comment on dealbreaker words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, word):\n",
    "    pattern = re.compile(r'([\\S\\s]*)(\\b{})([\\S\\s]*)'.format(word))\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    return None\n",
    "\n",
    "def split_Runs(doc,word):\n",
    "    for p in doc.paragraphs:\n",
    "        if p.text.find(word) != -1:\n",
    "            virtualRuns=p.runs\n",
    "            p.text = \"\"\n",
    "            for r in virtualRuns:\n",
    "                if r.text.find(word) != -1:\n",
    "                    before, word, after = split_text(r.text, word)\n",
    "                    p.add_run(before)\n",
    "                    p.add_run()\n",
    "                    p.add_run(word)\n",
    "                    p.add_run(after)\n",
    "                else:\n",
    "                    p.add_run(r.text)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def style_Token2(doc,word, dictionary1, comment=True):\n",
    "    for p in doc.paragraphs:\n",
    "        for i,r in enumerate(p.runs):\n",
    "            if p.runs[i].text.find(word) != -1:\n",
    "                p.runs[i].font.highlight_color = WD_COLOR_INDEX.RED\n",
    "                if comment:\n",
    "                    corresponded_key = find_key_by_value(word,dictionary1)\n",
    "                    p.runs[i-1].add_comment(f'Wanring: \\\"{word}\\\" is classified as a category {corresponded_key} dealbreaker word.\\nStrongly Advice its Immediate Removal',author=\"DG1\")\n",
    "\n",
    "\n",
    "    return doc\n",
    "\n",
    "\n",
    "# check_list\n",
    "\n",
    "for keyword in check_list:\n",
    "    doc=split_Runs(doc,keyword)    \n",
    "\n",
    "for keyword in check_list:\n",
    "    doc=style_Token2(doc,keyword, dealbreaker_words,True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving Comment on Representative and Remedies <br>\n",
    "if they exist, nothing will happen <br>\n",
    "if they do not exist, output a warning comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_existence_for_remedy_representative(doc,warning_1 = True, warning_2 = True):\n",
    "    if warning_1:\n",
    "        paragraph1 = doc.add_paragraph()\n",
    "        comment1 = paragraph1.add_comment(\"Warning: This document is lack of Representative Category information\", author = 'DG1')\n",
    "    if warning_2:\n",
    "        paragraph2 = doc.add_paragraph()\n",
    "        comment2 = paragraph2.add_comment(\"Warning: This document is lack of Remedy Category information\",author = 'DG1')\n",
    "\n",
    "\n",
    "    return doc\n",
    "\n",
    "doc = check_existence_for_remedy_representative(doc,representative_warning,remedies_warning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Path<br>\n",
    "输出路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##           ###           ##  EEEEEEEEEEEEEEEE      III\n",
    " ##         ## ##         ##   E                     III\n",
    "  ##       ##   ##       ##    E                     III\n",
    "   ##     ##     ##     ##     EEEEEEEEEEEEEEE       III\n",
    "    ##   ##       ##   ##      E                     III\n",
    "     ## ##         ## ##       E                     III\n",
    "      ##            ##         EEEEEEEEEEEEEEEE      III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改你的输出路径， 不要动r, \\\\, docx\n",
    "doc.save(r'C:\\\\Users\\\\dell-pc\\\\Desktop\\\\Only_NDA2.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 5.43 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total time taken: {total_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
