{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# pip install python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required import\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import docx2txt\n",
    "import spacy\n",
    "import re\n",
    "from docx import Document\n",
    "\n",
    "from docx.shared import RGBColor\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "\n",
    "import docx\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Synonyms of words in dealbreaker words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to find synonyms from the dealbreaker words list\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words in Parties \n",
    "\n",
    "Parties = [\"Unilateral\", \"One-Way\", \"One Way\", \"company (\\\"Disclosing Party\\\")\", \"company (\\\"Discloser\\\")\", \"corporation (\\\"Disclosing Party\\\")\", \"corporation (\\\"Discloser\\\")\",  \"LLC (\\\"Disclosing Party\\\")\", \"LLC (\\\"Discloser\\\")\",  \"Inc. (\\\"Disclosing Party\\\")\", \"Inc. (\\\"Discloser\\\")\",  \"Incorporated (\\\"Disclosing Party\\\")\", \"Incorporated (\\\"Discloser\\\")\", \"Co. (\\\"Disclosing Party\\\")\", \"Co. (\\\"Discloser\\\")\"]\n",
    "\n",
    "# No change because these words really do not have synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words in Residuals/ Memories\n",
    "\n",
    "Residuals = [\"residual\", \"residuals\", \"memories\", \"unaided\", \"memory\"]\n",
    "\n",
    "# find synonyms\n",
    "synonyms_residuals = []\n",
    "for word in Residuals:\n",
    "    synonyms_residuals.append(get_synonyms(word))\n",
    "\n",
    "## synonyms_residuals\n",
    "\n",
    "# since each word in Residuals will generate a list of its synonyms,\n",
    "# # hence, combine these sublists into one list\n",
    "combined_synonyms_residuals = []\n",
    "for sublst in synonyms_residuals:\n",
    "    combined_synonyms_residuals.extend(sublst)\n",
    "\n",
    "# clean the list to only pick up unique words, since some words' synonyms may be repeated\n",
    "unique_synonyms_residuals = list(set(combined_synonyms_residuals))\n",
    "\n",
    "\n",
    "# Notice: \n",
    "# There are TWO lists of words of Residuals now:\n",
    "# Only is only about Residuals Dealbreaker words\n",
    "# Second one is about synonyms of Residuals Dealbreaker words.\n",
    "\n",
    "\n",
    "## unique_synonyms_residuals.extend(Residuals)\n",
    "## unique_synonyms_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arealdy existed words in Limitation of Liability\n",
    "\n",
    "Limitation = [\"Limitation of Liability\", \"under no circumstances\", \"shall be limited\", \"special, incidental, indirect or consequential damages\", \"punitive\",\"exemplary\",\"consequential\",\"indirect\",\"incidental\"]\n",
    "words_in_limitation = [\"punitive\",\"exemplary\",\"consequential\",\"indirect\",\"incidental\"]\n",
    "\n",
    "# find synonyms\n",
    "synonyms_limitation = []\n",
    "for word in words_in_limitation:\n",
    "    synonyms_limitation.append(get_synonyms(word))\n",
    "\n",
    "## synonyms_limitation\n",
    "\n",
    "# combined sublists into one list\n",
    "combined_synonyms_limitation = []\n",
    "for sublst in synonyms_limitation:\n",
    "    combined_synonyms_limitation.extend(sublst)\n",
    "\n",
    "# unique words, some have repeated words\n",
    "unique_synonyms_limitation = list(set(combined_synonyms_limitation))\n",
    "\n",
    "# Notice: \n",
    "# There are TWO lists of words of Limitation of Liability now:\n",
    "# Only is only about Limitation of Liability Dealbreaker words\n",
    "# Second one is about synonyms of Limitation of Liability Dealbreaker words.\n",
    "\n",
    "\n",
    "## unique_synonyms_limitation.extend(Limitation)\n",
    "## unique_synonyms_limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words in Non-competition\n",
    "\n",
    "Noncompetition = [\"compete\", \"competition\", \"non-compete\", \"non-competition\", \"non compete\", \"non competition\" ]\n",
    "\n",
    "# find synonyms\n",
    "synonyms_noncompetition = []\n",
    "for word in Noncompetition:\n",
    "    synonyms_noncompetition.append(get_synonyms(word))\n",
    "\n",
    "## synonyms_noncompetition\n",
    "\n",
    "# combined sublists into one list\n",
    "combined_synonyms_noncompetition = []\n",
    "for sublst in synonyms_noncompetition:\n",
    "    combined_synonyms_noncompetition.extend(sublst)\n",
    "\n",
    "# unique words, some have repeated words\n",
    "unique_synonyms_noncompetition = list(set(combined_synonyms_noncompetition))\n",
    "\n",
    "\n",
    "\n",
    "# Notice: \n",
    "# There are TWO lists of words of Noncompetition now:\n",
    "# Only is only about Residuals Noncompetition words\n",
    "# Second one is about synonyms of Noncompetition Dealbreaker words.\n",
    "\n",
    "\n",
    "\n",
    "## unique_synonyms_noncompetition.extend(Noncompetition)\n",
    "## unique_synonyms_noncompetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words in Non-solicitation\n",
    "\n",
    "Nonsolicitation = [\"non-solicitation\", \"solicit\", \"non-solicit\", \"non-servicing\",   \"nonsolicitation\", \"nonsolicit\", \"nonservicing\",  \"solicit\",  \"non solicitation\", \"non solicit\", \"non servicing\",  \"no solicit\", \"no-solicit\"]\n",
    "\n",
    "# find synonyms\n",
    "synonyms_nonsolicitation = []\n",
    "for word in Nonsolicitation:\n",
    "    synonyms_nonsolicitation.append(get_synonyms(word))\n",
    "\n",
    "## synonyms_nonsolicitation\n",
    "\n",
    "# combined sublists into one list\n",
    "combined_synonyms_nonsolicitation = []\n",
    "for sublst in synonyms_nonsolicitation:\n",
    "    combined_synonyms_nonsolicitation.extend(sublst)\n",
    "\n",
    "# unique words, some have repeated words\n",
    "unique_synonyms_nonsolicitation = list(set(combined_synonyms_nonsolicitation))\n",
    "\n",
    "\n",
    "# Notice: \n",
    "# There are TWO lists of words of Nonsolicitation now:\n",
    "# Only is only about Residuals Dealbreaker words\n",
    "# Second one is about synonyms of Residuals Dealbreaker words.\n",
    "\n",
    "\n",
    "## unique_synonyms_nonsolicitation.extend(Nonsolicitation)\n",
    "## unique_synonyms_nonsolicitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words in Indemnification\n",
    "\n",
    "Indemnification = [\"indemnification\", \"indemnity\", \"hold-harmless\", \"hold harmless\", \"indemnify\", \"indemnified\", \"indemnifying\", \"defend\"]\n",
    "\n",
    "# find synonyms\n",
    "synonyms_indemnification = []\n",
    "for word in Indemnification:\n",
    "    synonyms_indemnification.append(get_synonyms(word))\n",
    "\n",
    "## synonyms_indemnification\n",
    "\n",
    "# combined sublists into one list\n",
    "combined_synonyms_indemnification = []\n",
    "for sublst in synonyms_indemnification:\n",
    "    combined_synonyms_indemnification.extend(sublst)\n",
    "\n",
    "# unique words, some have repeated words\n",
    "unique_synonyms_indemnification = list(set(combined_synonyms_indemnification))\n",
    "\n",
    "# Notice: \n",
    "# There are TWO lists of words of Indemnification now:\n",
    "# Only is only about Indemnification Dealbreaker words\n",
    "# Second one is about synonyms of Indemnification Dealbreaker words.\n",
    "\n",
    "\n",
    "## unique_synonyms_indemnification.extend(Indemnification)\n",
    "## unique_synonyms_indemnification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words in Governing Law/ Jurisdiction\n",
    "\n",
    "# No change, these words are only country names.\n",
    "\n",
    "Governing = [\"Texas\", \"Italy\", \"Italian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words in Exceptions\n",
    "\n",
    "# No changes, they are not words but lines.\n",
    "Exceptions =  [\"was communicated by Disclosing Party to an unaffiliated third party free of any obligation of confidence\",   \"was communicated by Disclosing Party to a third party free of any obligation of confidence\", \"was communicated by Disclosing Party to  a third party without an obligation of confidence\", \"was disclosed by Disclosing Party to  a third party without an obligation of confidence\", \" was disclosed by Discloser to a third party without an obligation of confidentiality\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words for Representatives\n",
    "\n",
    "# Different from the above, this part is for existence, if exist, yes, if not exist, output a warning sign about \"Missing Representatives\"\n",
    "\n",
    "Representatives = [\"responsible\", \"liable\"]\n",
    "\n",
    "# find synonyms\n",
    "synonyms_representatives = []\n",
    "for word in Representatives:\n",
    "    synonyms_representatives.append(get_synonyms(word))\n",
    "\n",
    "## synonyms_representatives\n",
    "\n",
    "# combined sublists into one list\n",
    "combined_synonyms_representatives = []\n",
    "for sublst in synonyms_representatives:\n",
    "    combined_synonyms_representatives.extend(sublst)\n",
    "\n",
    "# unique words, some have repeated words\n",
    "unique_synonyms_representatives = list(set(combined_synonyms_representatives))\n",
    "\n",
    "\n",
    "# Notice: \n",
    "# There are TWO lists of words of Representatives now:\n",
    "# Only is only about Representatives Dealbreaker words\n",
    "# Second one is about synonyms of Representatives Dealbreaker words.\n",
    "\n",
    "\n",
    "## unique_synonyms_representatives.extend(Representatives)\n",
    "## unique_synonyms_representatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words for Remedies\n",
    "\n",
    "# Same as the last one, this part is for existence, if exist, yes, if not exist, output a warning sign about \"Missing Remedies\"\n",
    "Remedies = [\"injunction\", \"injunctive\",\"equitable\"]\n",
    "\n",
    "# find synonyms\n",
    "synonyms_remedies = []\n",
    "for word in Remedies:\n",
    "    synonyms_remedies.append(get_synonyms(word))\n",
    "\n",
    "## synonyms_remedies\n",
    "\n",
    "# combined sublists into one list\n",
    "combined_synonyms_remedies = []\n",
    "for sublst in synonyms_remedies:\n",
    "    combined_synonyms_remedies.extend(sublst)\n",
    "\n",
    "# unique words, some have repeated words\n",
    "unique_synonyms_remedies = list(set(combined_synonyms_remedies))\n",
    "\n",
    "\n",
    "# Notice: \n",
    "# There are TWO lists of words of Remedies now:\n",
    "# Only is only about Remedies Dealbreaker words\n",
    "# Second one is about synonyms of Remedies Dealbreaker words.\n",
    "\n",
    "\n",
    "## unique_synonyms_remedies.extend(Remedies)\n",
    "## unique_synonyms_remedies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existed words for Privacy / Personal Information\n",
    "Privacy = [\"Personal Data\", \"Personal Information\",\"PII\", \"GDPR\", \"CCPA\", \"CPRA\", \"Privacy\"]\n",
    "words_in_privacy = [\"Privacy\"]\n",
    "\n",
    "# find synonyms\n",
    "synonyms_privacy = []\n",
    "for word in words_in_privacy:\n",
    "    synonyms_privacy.append(get_synonyms(word))\n",
    "\n",
    "## synonyms_privacy\n",
    "\n",
    "# combined sublists into one list\n",
    "combined_synonyms_privacy = []\n",
    "for sublst in synonyms_privacy:\n",
    "    combined_synonyms_privacy.extend(sublst)\n",
    "\n",
    "# unique words, some have repeated words\n",
    "unique_synonyms_privacy = list(set(combined_synonyms_privacy))\n",
    "\n",
    "# Notice: \n",
    "# There are TWO lists of words of Privacy now:\n",
    "# Only is only about Privacy Dealbreaker words\n",
    "# Second one is about synonyms of Privacy Dealbreaker words.\n",
    "\n",
    "\n",
    "## unique_synonyms_privacy.extend(Privacy)\n",
    "## unique_synonyms_privacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Trials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个地方 我其实也不需要输入路径， Test 3 是给weijian留着用来 直接放他的模型出来的dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section IS ONLY TEST!!!!!!!!!!!!!!\n",
    "\n",
    "# Copy the citation here\n",
    "\n",
    "# Test 1\n",
    "#paragraphs = \"3. Exclusions.  The Obligations of confidentiality exemplary assumed by the court Receiving Party pursuant to this Agreement shall not nonresistant apply to any information which (i) was available to the public at the time of disclosure or which subsequently becomes available to the public through no fault of Receiving Party; (ii) was in Receiving Party’s possession without any confidential oppose obligation prior to its receipt as evidenced by written records; (iii) is required to be disclosed exemplary by law, One-Way provided that Receiving Party promptly notifies Disclosing Party of such requirement to enable Disclosing Party the opportunity to Oppose such requirement or seek a protective order; and (iv) was independently developed by the Receiving Party without use of, or reliance upon, any Confidential Information, as evidenced by the Receiving Party’s written records. It is understood for the purposes of this Agreement that specific information disclosed by either Party to the other shall not be deemed to be available to the public or in the possession of Receiving Party merely because the specific information is embraced by more general information available to the public.  Also, any combination of features or elements relating to Confidential Information shall not be memory deemed to just be within the foregoing enumerated exclusions merely because the individual features are separately in the public domain or in the possession of Receiving Party unless the combination itself or its principle of operation were known publicly or were in the possession of Receiving Party prior to the disclosure of thereof by Disclosing Party.  The burden shall be on Receiving Party to responsible prove One Way applicability of any such exclusions.\"\n",
    "#paragraphs = \"The Receiving Party acknowledges that the Confidential Information is the property of the Disclosing Party and agrees to maintain the confidentiality of the Confidential Information. The Receiving Party further agrees to use the Confidential Information only for the purpose of evaluating a potential business relationship between the parties on a One-Way basis, and not to disclose any portion of the Confidential Information to any third party without the prior written consent of the Disclosing Party.\"\n",
    "\n",
    "# Test 2\n",
    "#doc = docx.Document('C:\\\\Users\\dell-pc\\Desktop\\simpletest.docx')\n",
    "\n",
    "#paragraphs = ''\n",
    "\n",
    "#for para in doc.paragraphs:\n",
    " #   paragraphs += para.text\n",
    "\n",
    "# Test 3\n",
    "\n",
    "\n",
    "original_input_dictionary = {\"The receiving party hereby acknowledges that the confidential information disclosed by the disclosing party shall be limited to the purpose of evaluating the feasibility of a potential business collaboration between the parties. The receiving party shall use its best efforts to maintain the confidentiality of the disclosed information and take all necessary measures to prevent unauthorized disclosure. Ensuant from any breach of this agreement, the disclosing party shall be entitled to seek legal remedies and damages to the fullest extent permitted by law.\" : \"RIG\",\n",
    "                             \"The receiving party acknowledges that it may have access to the disclosing party's personally identifiable information (PII) during the term of this agreement. The receiving party shall maintain the confidentiality of the PII and shall not use or disclose it to any third party, except as necessary to perform its obligations under this agreement. The receiving party shall implement and maintain adequate measures to safeguard the PII from unauthorized access, use, disclosure, or concealment. In the event of any breach or suspected breach of the PII confidentiality, the receiving party shall promptly notify the disclosing party and take all necessary measures to mitigate the damages and prevent further unauthorized disclosure.\":\"TER\"}\n",
    "\n",
    "paragraphs = list(original_input_dictionary.keys())\n",
    "#len(paragraphs)\n",
    "\n",
    "# Test 4\n",
    "#paragraphs = \"The receiving party shall have no obligation under this Agreement to maintain in confidence any information which it can demonstrate by documentary evidence: (i)  is disclosed in a tangible medium available to the public; or (ii)  is otherwise in the public domain at the time of disclosure or subsequently becomes part of the public domain through no fault of the receiving party or persons or entities to whom the receiving party has disclosed such information; or (iii)  is lawfully in the possession of the receiving party prior to the time of disclosure by the disclosing party and is not subject to any duty of confidentiality; or (iv)  the receiving party lawfully obtains from any third party not under any obligation to keep such information confidential.  \" \n",
    "\n",
    "# Test 5\n",
    "#with open(r\"C:\\\\Users\\dell-pc\\Desktop\\1.txt\") as file:\n",
    "   # paragraphs = file.read()\n",
    "\n",
    "### paragraphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment paragraph into sentences. \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentences_in_paragraphs = []  \n",
    "\n",
    "# take unicode string  \n",
    "for i in range(len(paragraphs)):\n",
    "    sentences_in_paragraph = nlp(paragraphs[i])\n",
    "\n",
    "    sentencess = list(sentences_in_paragraph.sents)\n",
    "\n",
    "    separated_sentences = []\n",
    "    # list all sentences\n",
    "    for j in range(len(sentencess)):\n",
    "        sentence = sentencess[j].text\n",
    "    \n",
    "    \n",
    "        sentence = sentence.replace('\\n\\n', ' ')\n",
    "        sentences = re.split(r'\\.(?=\\s|$)', sentence)\n",
    "\n",
    "\n",
    "        separated_sentences += sentences\n",
    "\n",
    "    final_separated_sentences = [x for x in separated_sentences if x != '']\n",
    "\n",
    "    sentences_in_paragraphs.append(final_separated_sentences)\n",
    "\n",
    "## sentences_in_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n正确版本\\n# Segment paragraph into sentences. \\nnlp = spacy.load(\"en_core_web_sm\")\\n\\n# take unicode string  \\nsentences = nlp(paragraphs)\\n\\n# to print sentences\\n\\n##for sent in sentences.sents:\\n ##     print(sent)\\n\\n\\nsentencess = list(sentences.sents)\\n\\nseparated_sentences = []\\n\\n# list all sentences\\nfor i in range(len(sentencess)):\\n    sentence = sentencess[i].text\\n    \\n    #sentence_separation = re.split(\\'\\t|\\n|\\n\\t\\', sentence)\\n    \\n    sentence = sentence.replace(\\'\\n\\n\\', \\' \\')\\n    sentences = re.split(r\\'\\\\.(?=\\\\s|$)\\', sentence)\\n\\n\\n    separated_sentences += sentences\\n    #print(sentence)\\n\\nfinal_separated_sentences = [x for x in separated_sentences if x != \\'\\']\\n## final_separated_sentences\\n'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "正确版本\n",
    "# Segment paragraph into sentences. \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# take unicode string  \n",
    "sentences = nlp(paragraphs)\n",
    "\n",
    "# to print sentences\n",
    "\n",
    "##for sent in sentences.sents:\n",
    " ##     print(sent)\n",
    "\n",
    "\n",
    "sentencess = list(sentences.sents)\n",
    "\n",
    "separated_sentences = []\n",
    "\n",
    "# list all sentences\n",
    "for i in range(len(sentencess)):\n",
    "    sentence = sentencess[i].text\n",
    "    \n",
    "    #sentence_separation = re.split('\\t|\\n|\\n\\t', sentence)\n",
    "    \n",
    "    sentence = sentence.replace('\\n\\n', ' ')\n",
    "    sentences = re.split(r'\\.(?=\\s|$)', sentence)\n",
    "\n",
    "\n",
    "    separated_sentences += sentences\n",
    "    #print(sentence)\n",
    "\n",
    "final_separated_sentences = [x for x in separated_sentences if x != '']\n",
    "## final_separated_sentences\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary for dealbreaker catergories and words with each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall in previous section, we have already find synonyms of dealbreaker words and list them. \n",
    "\n",
    "## Parties\n",
    "## unique_synonyms_residuals\n",
    "## unique_synonyms_limitation\n",
    "## unique_synonyms_noncompetition\n",
    "## unique_synonyms_nonsolicitation\n",
    "## unique_synonyms_indemnification\n",
    "## Governing\n",
    "## Exceptions\n",
    "## unique_synonyms_representatives\n",
    "## unique_synonyms_remedies\n",
    "## unique_synonyms_privacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of sensitive words, grouped them by different categories. \n",
    "\n",
    "dealbreaker_words = {\n",
    "    \"Parties\":Parties,\n",
    "     \"Residuals/Memories\": Residuals,\n",
    "     \"Limitation of Liability\": Limitation,\n",
    "     \"Non-competition\": Noncompetition,\n",
    "     \"Non-solicitation\":Nonsolicitation,\n",
    "     \"Indemnification\": Indemnification,\n",
    "     \"Governing Law/Jurisdiction\":Governing,\n",
    "     \"Exceptions\":Exceptions,\n",
    "     \"Privacy\": Privacy\n",
    "}\n",
    "\n",
    "# The reason why not including remedies and representatives is due to the reason that\n",
    "# their search methods are for existence (recall if they exist, then everything is ok, but if not exist, Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of synonyms of sensitive words, grouped them by different categories. \n",
    "\n",
    "synonyms_dealbreaker_words = {\n",
    "     \"Synonyms of Residuals/Memories\": unique_synonyms_residuals,\n",
    "     \"Synonyms of Limitation of Liability\": unique_synonyms_limitation,\n",
    "     \"Synonyms of Non-competition\": unique_synonyms_noncompetition,\n",
    "     \"Synonyms of Non-solicitation\":unique_synonyms_nonsolicitation,\n",
    "     \"Synonyms of Indemnification\": unique_synonyms_indemnification,\n",
    "     \"Synonyms of Privacy\": unique_synonyms_privacy\n",
    "}\n",
    "\n",
    "# The reason why not including remedies and representatives is due to the reason that\n",
    "# their search methods are for existence (recall if they exist, then everything is ok, but if not exist, Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_dealbreaker_words = {\n",
    "    \"DEF\": Parties ,\n",
    "    \"RIG\": Limitation,\n",
    "    \"EXP\": Exceptions,\n",
    "    \"WAR\": Parties + Residuals + Limitation + Noncompetition + Nonsolicitation + Indemnification + Governing + Exceptions + Privacy,\n",
    "    \"GOV\": Governing,\n",
    "    \"REM\": Remedies,\n",
    "    \"TER\": Parties + Residuals + Limitation + Noncompetition + Nonsolicitation + Indemnification + Governing + Exceptions + Privacy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_synonyms_dealbreaker_words = {\n",
    "    \"RIG\": unique_synonyms_limitation,\n",
    "    \"WAR\": unique_synonyms_residuals + unique_synonyms_limitation + unique_synonyms_noncompetition + unique_synonyms_nonsolicitation + unique_synonyms_indemnification  + unique_synonyms_privacy,\n",
    "    \"REM\": unique_synonyms_remedies,\n",
    "    \"TER\": unique_synonyms_residuals + unique_synonyms_limitation + unique_synonyms_noncompetition + unique_synonyms_nonsolicitation + unique_synonyms_indemnification  + unique_synonyms_privacy\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find each paragraph's dealbreaker words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_specific_dealbreakers(input_key, input_dictionary):\n",
    "    for i in range(len(list(input_dictionary.keys()))):\n",
    "        if input_key in list(input_dictionary.keys())[i]: \n",
    "            new_dict = {list(input_dictionary.keys())[i]: input_dictionary[list(input_dictionary.keys())[i]] }\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = find_specific_dealbreakers(list(original_input_dictionary.values())[0], update_dealbreaker_words)\n",
    "\n",
    "## new_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First about dealbreaker words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shall be limited', 'PII', 'PII', 'PII', 'PII']"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_check_list = []\n",
    "\n",
    "for j in range(len(paragraphs)):\n",
    "    #check_list = []\n",
    "\n",
    "    new_dict = find_specific_dealbreakers(list(original_input_dictionary.values())[j], update_dealbreaker_words)\n",
    "\n",
    "    for i in range(len(sentences_in_paragraphs[j])):\n",
    "        sentence = sentences_in_paragraphs[j][i]\n",
    "        # Iterate through the sensitive words grouped by category\n",
    "        for category, words in new_dict.items():\n",
    "            # Use regular expressions to search for the words in the sentence\n",
    "            for word in words:\n",
    "                matches = re.finditer(r'\\b'+word+r'\\b', sentence, re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                #paragraph_position = re.search(r'\\d+',sentence)\n",
    "                #section_position = re.search(r'\\.(\\d+)',sentence)\n",
    "\n",
    "                #print(f\"\\nDealbreaker word \\\"{word}\\\" is detected in at sentence \\n\\\"{sentence}\\\"      \\n\")\n",
    "                #print(f\"Dealbreaker word \\\"{word}\\\" is detected in Subtitle {paragraph_position.group()} Section {section_position.group(1)} at sentence \\n\\\"{sentence}\\\"      \\n\")\n",
    "                #print(f\"The word '{word}' belongs to category '{category}' and this dealbreaker word appears at position {match.start()+1} in sentence {i+1}\\n\")\n",
    "                #print(\"__________________________________________________________________________________________________________________________________________________________________________________________________________________________-\")\n",
    "                #print(f\"{match}\")\n",
    "                    combined_check_list.append(match.group())\n",
    "\n",
    "\n",
    "combined_check_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncheck_list = []\\nfor i in range(len(separated_sentences)):\\n    sentence = separated_sentences[i]\\n# Iterate through the sensitive words grouped by category\\n    for category, words in dealbreaker_words.items():\\n        # Use regular expressions to search for the words in the sentence\\n        for word in words:\\n            matches = re.finditer(r\\'\\x08\\'+word+r\\'\\x08\\', sentence, re.IGNORECASE)\\n            for match in matches:\\n                #paragraph_position = re.search(r\\'\\\\d+\\',sentence)\\n                #section_position = re.search(r\\'\\\\.(\\\\d+)\\',sentence)\\n\\n                #print(f\"\\nDealbreaker word \"{word}\" is detected in at sentence \\n\"{sentence}\"      \\n\")\\n                #print(f\"Dealbreaker word \"{word}\" is detected in Subtitle {paragraph_position.group()} Section {section_position.group(1)} at sentence \\n\"{sentence}\"      \\n\")\\n                #print(f\"The word \\'{word}\\' belongs to category \\'{category}\\' and this dealbreaker word appears at position {match.start()+1} in sentence {i+1}\\n\")\\n                #print(\"__________________________________________________________________________________________________________________________________________________________________________________________________________________________-\")\\n                #print(f\"{match}\")\\n                check_list.append(match.group())\\n\\ncheck_list\\n'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "check_list = []\n",
    "for i in range(len(separated_sentences)):\n",
    "    sentence = separated_sentences[i]\n",
    "# Iterate through the sensitive words grouped by category\n",
    "    for category, words in dealbreaker_words.items():\n",
    "        # Use regular expressions to search for the words in the sentence\n",
    "        for word in words:\n",
    "            matches = re.finditer(r'\\b'+word+r'\\b', sentence, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                #paragraph_position = re.search(r'\\d+',sentence)\n",
    "                #section_position = re.search(r'\\.(\\d+)',sentence)\n",
    "\n",
    "                #print(f\"\\nDealbreaker word \\\"{word}\\\" is detected in at sentence \\n\\\"{sentence}\\\"      \\n\")\n",
    "                #print(f\"Dealbreaker word \\\"{word}\\\" is detected in Subtitle {paragraph_position.group()} Section {section_position.group(1)} at sentence \\n\\\"{sentence}\\\"      \\n\")\n",
    "                #print(f\"The word '{word}' belongs to category '{category}' and this dealbreaker word appears at position {match.start()+1} in sentence {i+1}\\n\")\n",
    "                #print(\"__________________________________________________________________________________________________________________________________________________________________________________________________________________________-\")\n",
    "                #print(f\"{match}\")\n",
    "                check_list.append(match.group())\n",
    "\n",
    "check_list\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ensuant', 'maintain', 'maintain', 'concealment', 'damages']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_check_list_syn = []\n",
    "\n",
    "for j in range(len(paragraphs)):\n",
    "    new_dict = find_specific_dealbreakers(list(original_input_dictionary.values())[j], update_synonyms_dealbreaker_words)\n",
    "\n",
    "    for i in range(len(sentences_in_paragraphs[j])):\n",
    "        sentence = sentences_in_paragraphs[j][i]\n",
    "# Iterate through the sensitive words grouped by category\n",
    "        for category, words in new_dict.items():\n",
    "            # Use regular expressions to search for the words in the sentence\n",
    "            for word in words:\n",
    "                matches = re.finditer(r'\\b'+word+r'\\b', sentence, re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    #paragraph_position = re.search(r'\\d+',sentence)\n",
    "                    #section_position = re.search(r'\\.(\\d+)',sentence)\n",
    "\n",
    "                    #print(f\"\\nDealbreaker word \\\"{word}\\\" is detected in at sentence \\n\\\"{sentence}\\\"      \\n\")\n",
    "                    #print(f\"Dealbreaker word \\\"{word}\\\" is detected in Subtitle {paragraph_position.group()} Section {section_position.group(1)} at sentence \\n\\\"{sentence}\\\"      \\n\")\n",
    "                    #print(f\"The word '{word}' belongs to category '{category}' and this dealbreaker word appears at position {match.start()+1} in sentence {i+1}\\n\")\n",
    "                    #print(\"__________________________________________________________________________________________________________________________________________________________________________________________________________________________-\")\n",
    "                    #print(f\"{match}\")\n",
    "                    combined_check_list_syn.append(match.group())\n",
    "\n",
    "combined_check_list_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncheck_list_syn = []\\nfor i in range(len(separated_sentences)):\\n    sentence = separated_sentences[i]\\n# Iterate through the sensitive words grouped by category\\n    for category, words in synonyms_dealbreaker_words.items():\\n        # Use regular expressions to search for the words in the sentence\\n        for word in words:\\n            matches = re.finditer(r\\'\\x08\\'+word+r\\'\\x08\\', sentence, re.IGNORECASE)\\n            for match in matches:\\n                #paragraph_position = re.search(r\\'\\\\d+\\',sentence)\\n                #section_position = re.search(r\\'\\\\.(\\\\d+)\\',sentence)\\n\\n                #print(f\"\\nDealbreaker word \"{word}\" is detected in at sentence \\n\"{sentence}\"      \\n\")\\n                #print(f\"Dealbreaker word \"{word}\" is detected in Subtitle {paragraph_position.group()} Section {section_position.group(1)} at sentence \\n\"{sentence}\"      \\n\")\\n                #print(f\"The word \\'{word}\\' belongs to category \\'{category}\\' and this dealbreaker word appears at position {match.start()+1} in sentence {i+1}\\n\")\\n                #print(\"__________________________________________________________________________________________________________________________________________________________________________________________________________________________-\")\\n                #print(f\"{match}\")\\n                check_list_syn.append(match.group())\\n\\n## check_list_syn\\n\\n'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "check_list_syn = []\n",
    "for i in range(len(separated_sentences)):\n",
    "    sentence = separated_sentences[i]\n",
    "# Iterate through the sensitive words grouped by category\n",
    "    for category, words in synonyms_dealbreaker_words.items():\n",
    "        # Use regular expressions to search for the words in the sentence\n",
    "        for word in words:\n",
    "            matches = re.finditer(r'\\b'+word+r'\\b', sentence, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                #paragraph_position = re.search(r'\\d+',sentence)\n",
    "                #section_position = re.search(r'\\.(\\d+)',sentence)\n",
    "\n",
    "                #print(f\"\\nDealbreaker word \\\"{word}\\\" is detected in at sentence \\n\\\"{sentence}\\\"      \\n\")\n",
    "                #print(f\"Dealbreaker word \\\"{word}\\\" is detected in Subtitle {paragraph_position.group()} Section {section_position.group(1)} at sentence \\n\\\"{sentence}\\\"      \\n\")\n",
    "                #print(f\"The word '{word}' belongs to category '{category}' and this dealbreaker word appears at position {match.start()+1} in sentence {i+1}\\n\")\n",
    "                #print(\"__________________________________________________________________________________________________________________________________________________________________________________________________________________________-\")\n",
    "                #print(f\"{match}\")\n",
    "                check_list_syn.append(match.group())\n",
    "\n",
    "## check_list_syn\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean it for unique words, Upper or lower case matters!\n",
    "\n",
    "combined_check_list = list(set(combined_check_list))\n",
    "combined_check_list_syn = list(set(combined_check_list_syn))\n",
    "\n",
    "## combined_check_list_syn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, about representative and rememdies. <br>\n",
    "Here we need to find out whether they exist or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representatives\n",
    "representative_warning = True\n",
    "\n",
    "for j in range(len(paragraphs)):\n",
    "\n",
    "    for i in range(len(sentences_in_paragraphs[j])):\n",
    "        sentence = sentences_in_paragraphs[j][i]\n",
    "        \n",
    "        for word in Representatives:\n",
    "            \n",
    "            if word in sentence:\n",
    "                representative_warning = False\n",
    "                ## print(f\"Found '{word}' in sentence: {sentence}\")\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        break\n",
    "    \n",
    "    else:\n",
    "        representative_warning = True\n",
    "    ## print(\"None of the Representative words were found in any of the sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Representatives\\nfor i in range(len(separated_sentences)):\\n    sentence = separated_sentences[i]\\n    for word in Representatives:\\n        if word in sentence:\\n            representative_warning = False\\n            ## print(f\"Found \\'{word}\\' in sentence: {sentence}\")\\n            break\\n    else:\\n        continue\\n\\n    break\\nelse:\\n    representative_warning = True\\n    ## print(\"None of the Representative words were found in any of the sentences\")\\n'"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Representatives\n",
    "for i in range(len(separated_sentences)):\n",
    "    sentence = separated_sentences[i]\n",
    "    for word in Representatives:\n",
    "        if word in sentence:\n",
    "            representative_warning = False\n",
    "            ## print(f\"Found '{word}' in sentence: {sentence}\")\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    break\n",
    "else:\n",
    "    representative_warning = True\n",
    "    ## print(\"None of the Representative words were found in any of the sentences\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remedies\n",
    "remedies_warning = True\n",
    "\n",
    "for j in range(len(paragraphs)):\n",
    "\n",
    "    for i in range(len(sentences_in_paragraphs[j])):\n",
    "        sentence = sentences_in_paragraphs[j][i]\n",
    "        \n",
    "        for word in Remedies:\n",
    "            if word in sentence:\n",
    "                remedies_warning = False\n",
    "                ## print(f\"Found '{word}' in sentence: {sentence}\")\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        break\n",
    "    else:\n",
    "        remedies_warning = True\n",
    "        ## print(\"None of the Remedies words were found in any of the sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Remedies\\nfor i in range(len(separated_sentences)):\\n    sentence = separated_sentences[i]\\n    for word in Remedies:\\n        if word in sentence:\\n            remedies_warning = False\\n            ## print(f\"Found \\'{word}\\' in sentence: {sentence}\")\\n            break\\n    else:\\n        continue\\n\\n    break\\nelse:\\n    remedies_warning = True\\n    ## print(\"None of the Remedies words were found in any of the sentences\")\\n'"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Remedies\n",
    "for i in range(len(separated_sentences)):\n",
    "    sentence = separated_sentences[i]\n",
    "    for word in Remedies:\n",
    "        if word in sentence:\n",
    "            remedies_warning = False\n",
    "            ## print(f\"Found '{word}' in sentence: {sentence}\")\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    break\n",
    "else:\n",
    "    remedies_warning = True\n",
    "    ## print(\"None of the Remedies words were found in any of the sentences\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representatives Synonyms\n",
    "syn_representative_warning = True\n",
    "\n",
    "for j in range(len(paragraphs)):\n",
    "\n",
    "    for i in range(len(sentences_in_paragraphs[j])):\n",
    "        sentence = sentences_in_paragraphs[j][i]\n",
    "\n",
    "        for word in unique_synonyms_representatives:\n",
    "            if word in sentence:\n",
    "                syn_representative_warning = False\n",
    "                ## print(f\"Found '{word}' in sentence: {sentence}\")\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        break\n",
    "    else:\n",
    "        syn_representative_warning = True\n",
    "        ## print(\"None of the Representative words were found in any of the sentences\")\n",
    "\n",
    "\n",
    "if representative_warning == False:\n",
    "    syn_representative_warning == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Representatives Synonyms\\nfor i in range(len(separated_sentences)):\\n    sentence = separated_sentences[i]\\n    for word in unique_synonyms_representatives:\\n        if word in sentence:\\n            syn_representative_warning = False\\n            ## print(f\"Found \\'{word}\\' in sentence: {sentence}\")\\n            break\\n    else:\\n        continue\\n\\n    break\\nelse:\\n    syn_representative_warning = True\\n    ## print(\"None of the Representative words were found in any of the sentences\")\\n\\nif representative_warning == False:\\n    syn_representative_warning == False\\n'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Representatives Synonyms\n",
    "for i in range(len(separated_sentences)):\n",
    "    sentence = separated_sentences[i]\n",
    "    for word in unique_synonyms_representatives:\n",
    "        if word in sentence:\n",
    "            syn_representative_warning = False\n",
    "            ## print(f\"Found '{word}' in sentence: {sentence}\")\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    break\n",
    "else:\n",
    "    syn_representative_warning = True\n",
    "    ## print(\"None of the Representative words were found in any of the sentences\")\n",
    "\n",
    "if representative_warning == False:\n",
    "    syn_representative_warning == False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonyms Remedies\n",
    "syn_remedies_warning = True\n",
    "\n",
    "for j in range(len(paragraphs)):\n",
    "\n",
    "    for i in range(len(sentences_in_paragraphs[j])):\n",
    "        sentence = sentences_in_paragraphs[j][i]\n",
    "\n",
    "        for word in unique_synonyms_remedies:\n",
    "            if word in sentence:\n",
    "                syn_remedies_warning = False\n",
    "                ## print(f\"Found '{word}' in sentence: {sentence}\")\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        break\n",
    "    else:\n",
    "        syn_remedies_warning = True\n",
    "        ## print(\"None of the Remedies words were found in any of the sentences\")\n",
    "\n",
    "\n",
    "if remedies_warning == False:\n",
    "    syn_remedies_warning == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Synonyms Remedies\\nfor i in range(len(separated_sentences)):\\n    sentence = separated_sentences[i]\\n    for word in unique_synonyms_remedies:\\n        if word in sentence:\\n            syn_remedies_warning = False\\n            ## print(f\"Found \\'{word}\\' in sentence: {sentence}\")\\n            break\\n    else:\\n        continue\\n\\n    break\\nelse:\\n    syn_remedies_warning = True\\n    ## print(\"None of the Remedies words were found in any of the sentences\")\\n\\nif remedies_warning == False:\\n    syn_remedies_warning == False\\n'"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Synonyms Remedies\n",
    "for i in range(len(separated_sentences)):\n",
    "    sentence = separated_sentences[i]\n",
    "    for word in unique_synonyms_remedies:\n",
    "        if word in sentence:\n",
    "            syn_remedies_warning = False\n",
    "            ## print(f\"Found '{word}' in sentence: {sentence}\")\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    break\n",
    "else:\n",
    "    syn_remedies_warning = True\n",
    "    ## print(\"None of the Remedies words were found in any of the sentences\")\n",
    "\n",
    "if remedies_warning == False:\n",
    "    syn_remedies_warning == False\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把这个地方的输入路径改了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import document again!\n",
    "doc=docx.Document('C:\\\\Users\\dell-pc\\Desktop\\simpletest.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key_by_value(value, dictionary):\n",
    "    keys = []\n",
    "    for key, val in dictionary.items():\n",
    "        if isinstance(val, list):\n",
    "            if any(v.lower() == value.lower() for v in val):\n",
    "                keys.append(key)\n",
    "        elif val.lower() == value.lower():\n",
    "            keys.append(key)\n",
    "    return keys if keys else None  # if value is not found in any key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving comment on dealbreaker words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_text(text, word):\n",
    "    pattern = re.compile(r'([\\S\\s]*)(\\b{})([\\S\\s]*)'.format(word))\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    return None\n",
    "\n",
    "def split_Runs(doc,word):\n",
    "    for p in doc.paragraphs:\n",
    "        if p.text.find(word) != -1:\n",
    "            virtualRuns=p.runs\n",
    "            p.text = \"\"\n",
    "            for r in virtualRuns:\n",
    "                if r.text.find(word) != -1:\n",
    "                    before, word, after = split_text(r.text, word)\n",
    "                    p.add_run(before)\n",
    "                    p.add_run()\n",
    "                    p.add_run(word)\n",
    "                    p.add_run(after)\n",
    "                else:\n",
    "                    p.add_run(r.text)\n",
    "    return doc\n",
    "\n",
    "def style_Token(doc,word,dictionary,comment=True):\n",
    "    for p in doc.paragraphs:\n",
    "        for i,r in enumerate(p.runs):\n",
    "            if p.runs[i].text.find(word) != -1:\n",
    "                p.runs[i].font.highlight_color = WD_COLOR_INDEX.RED\n",
    "                if comment:\n",
    "                    corresponded_key = find_key_by_value(word,dictionary)\n",
    "                    p.runs[i-1].add_comment(f'Important: \\\"{word}\\\" is a dealbreaker word belonging to category {corresponded_key}',author=\"DG1\")\n",
    "                    #r.add_comment(f'{word} No se encuentra en el documento',author='BOT CONFRONT')\n",
    "    return doc\n",
    "\n",
    "def style_Token2(doc,word,list1, dictionary1,dictionary2, comment=True):\n",
    "    for p in doc.paragraphs:\n",
    "        for i,r in enumerate(p.runs):\n",
    "            if word in list1:\n",
    "                if p.runs[i].text.find(word) != -1:\n",
    "                    p.runs[i].font.highlight_color = WD_COLOR_INDEX.RED\n",
    "                    if comment:\n",
    "                        corresponded_key = find_key_by_value(word,dictionary1)\n",
    "                        p.runs[i-1].add_comment(f'Wanring: \\\"{word}\\\" is classified as a category {corresponded_key} dealbreaker word.\\nStrongly Advice its Immediate Removal',author=\"DG1\")\n",
    "                        #r.add_comment(f'{word} No se encuentra en el documento',author='BOT CONFRONT')\n",
    "            else:\n",
    "                if p.runs[i].text.find(word) != -1:\n",
    "                    p.runs[i].font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "                    if comment:\n",
    "                        corresponded_key = find_key_by_value(word,dictionary2)\n",
    "                        p.runs[i-1].add_comment(f'Suggestion: \\\"{word}\\\" is a synonym of dealbreaker word belonging to category {corresponded_key}. We Recommend that you carefully evaluate its usage before considering its reintroduction.',author=\"DG1\")\n",
    "                        #r.add_comment(f'{word} No se encuentra en el documento',author='BOT CONFRONT')\n",
    "\n",
    "    return doc\n",
    "\n",
    "#for keyword in check_list:\n",
    "   # doc=split_Runs(doc,keyword)\n",
    "#for keyword in check_list:\n",
    "    #doc=style_Token(doc,keyword,dealbreaker_words,True)\n",
    "\n",
    "dealbreaker_and_syn_list = combined_check_list + combined_check_list_syn\n",
    "dealbreaker_and_syn_list = list(set(dealbreaker_and_syn_list))\n",
    "dealbreaker_and_syn_list\n",
    "\n",
    "for keyword in dealbreaker_and_syn_list:\n",
    "    doc=split_Runs(doc,keyword)    \n",
    "\n",
    "for keyword in dealbreaker_and_syn_list:\n",
    "    doc=style_Token2(doc,keyword,combined_check_list, dealbreaker_words, synonyms_dealbreaker_words,True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving Comment on Representative and Remedies <br>\n",
    "if they exist, nothing will happen <br>\n",
    "if they do not exist, output a warning comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_existence_for_remedy_representative(doc,warning_1 = True, warning_2 = True, warning_3 = True, warning_4 = True):\n",
    "    if warning_1:\n",
    "        paragraph1 = doc.add_paragraph()\n",
    "        comment1 = paragraph1.add_comment(\"Warning: This document is lack of Representative Category information\", author = 'DG1')\n",
    "    if warning_2:\n",
    "        paragraph2 = doc.add_paragraph()\n",
    "        comment2 = paragraph2.add_comment(\"Warning: This document is lack of Remedy Category information\",author = 'DG1')\n",
    "    if warning_3:\n",
    "        paragraph3 = doc.add_paragraph()\n",
    "        comment3 = paragraph3.add_comment(\"Suggested Warning: This document is lack of synonyms of Representative Category information\", author = 'DG1')\n",
    "    if warning_4:\n",
    "        paragraph4 = doc.add_paragraph()\n",
    "        comment4 = paragraph4.add_comment(\"Suggested Warning: This document is lack of synonyms of Remedy Category information\",author = 'DG1')\n",
    "\n",
    "\n",
    "    return doc\n",
    "\n",
    "doc = check_existence_for_remedy_representative(doc,representative_warning,remedies_warning,syn_representative_warning, syn_remedies_warning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.save('C:\\\\Users\\dell-pc\\Desktop\\highlighted_document.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 6.08 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total time taken: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a596c01fbd375c02cbd8f063d934d64bec48590251991d1848d8d257727bfba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
