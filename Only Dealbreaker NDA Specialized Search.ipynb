{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":103073,"status":"ok","timestamp":1682517950416,"user":{"displayName":"Yichen Li","userId":"05769548336440927322"},"user_tz":240},"id":"6fNz2x1idrV-","outputId":"eedc243f-7d9f-4fc1-e4de-e5234a9d4d04"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pypandoc\n","  Downloading pypandoc-1.11-py3-none-any.whl (20 kB)\n","Installing collected packages: pypandoc\n","Successfully installed pypandoc-1.11\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting docx2txt\n","  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: docx2txt\n","  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3977 sha256=2f264062a0ba47df82485fc8c3339ed9a327c1fbaee820f21d568214376dfe6e\n","  Stored in directory: /root/.cache/pip/wheels/40/75/01/e6c444034338bde9c7947d3467807f889123465c2371e77418\n","Successfully built docx2txt\n","Installing collected packages: docx2txt\n","Successfully installed docx2txt-0.8\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting python-docx\n","  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from python-docx) (4.9.2)\n","Building wheels for collected packages: python-docx\n","  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=603d9ad8f41e780da31eb533a593733f57af61d656ffd4c77ccfadb71ce2cc07\n","  Stored in directory: /root/.cache/pip/wheels/83/8b/7c/09ae60c42c7ba4ed2dddaf2b8b9186cb105255856d6ed3dba5\n","Successfully built python-docx\n","Installing collected packages: python-docx\n","Successfully installed python-docx-0.8.11\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: python-docx in /usr/local/lib/python3.9/dist-packages (0.8.11)\n","Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from python-docx) (4.9.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting lxml==4.9.1\n","  Downloading lxml-4.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lxml\n","  Attempting uninstall: lxml\n","    Found existing installation: lxml 4.9.2\n","    Uninstalling lxml-4.9.2:\n","      Successfully uninstalled lxml-4.9.2\n","Successfully installed lxml-4.9.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting bayoo-docx\n","  Downloading bayoo_docx-0.2.20-py3-none-any.whl (193 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from bayoo-docx) (4.9.1)\n","Installing collected packages: bayoo-docx\n","Successfully installed bayoo-docx-0.2.20\n","Name: lxml\n","Version: 4.9.1\n","Summary: Powerful and Pythonic XML processing library combining libxml2/libxslt with the ElementTree API.\n","Home-page: https://lxml.de/\n","Author: lxml dev team\n","Author-email: lxml-dev@lxml.de\n","License: BSD\n","Location: /usr/local/lib/python3.9/dist-packages\n","Requires: \n","Required-by: bayoo-docx, nbconvert, pandas-datareader, python-docx, yfinance\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install pypandoc\n","!pip install transformers\n","!pip install docx2txt\n","!pip install python-docx \n","import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","!pip install python-docx\n","!pip install lxml==4.9.1\n","!pip install bayoo-docx\n","! pip show lxml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vg5KdlqfdrWA"},"outputs":[],"source":["# Required import\n","import nltk\n","from nltk.corpus import wordnet\n","\n","import docx2txt\n","import spacy\n","import re\n","from docx import Document\n","\n","from docx.shared import RGBColor\n","from docx.oxml import OxmlElement\n","from docx.oxml.ns import qn\n","\n","import docx\n","from docx.enum.text import WD_COLOR_INDEX\n","import time\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import pypandoc\n","import plotly.figure_factory as ff\n","import seaborn as sns\n","import tensorflow\n","from nltk.corpus import stopwords\n","from plotly.offline import iplot\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.initializers import TruncatedNormal\n","from tensorflow.keras.layers import Dense, Dropout, Input\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","from tensorflow.keras.metrics import CategoricalAccuracy\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import to_categorical\n","from transformers import BertConfig, BertTokenizerFast, TFBertModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRblmW_NdrWA"},"outputs":[],"source":["start_time = time.time()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pBRUosjJdrWB"},"outputs":[],"source":["# Already existed words in Parties \n","\n","Parties = [\"Unilateral\", \"One-Way\", \"One Way\", \"company (\\\"Disclosing Party\\\")\", \"company (\\\"Discloser\\\")\", \"corporation (\\\"Disclosing Party\\\")\", \"corporation (\\\"Discloser\\\")\",  \"LLC (\\\"Disclosing Party\\\")\", \"LLC (\\\"Discloser\\\")\",  \"Inc. (\\\"Disclosing Party\\\")\", \"Inc. (\\\"Discloser\\\")\",  \"Incorporated (\\\"Disclosing Party\\\")\", \"Incorporated (\\\"Discloser\\\")\", \"Co. (\\\"Disclosing Party\\\")\", \"Co. (\\\"Discloser\\\")\"]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3xOTcp0HdrWB"},"outputs":[],"source":["# Already existed words in Residuals/ Memories\n","\n","Residuals = [\"residual\", \"residuals\", \"memories\", \"unaided\", \"memory\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbnqpQK0drWB"},"outputs":[],"source":["# Arealdy existed words in Limitation of Liability\n","\n","Limitation = [\"Limitation of Liability\", \"under no circumstances\", \"shall be limited\", \"special, incidental, indirect or consequential damages\", \"punitive\",\"exemplary\",\"consequential\",\"indirect\",\"incidental\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ep4duB4udrWB"},"outputs":[],"source":["# Already existed words in Non-competition\n","\n","Noncompetition = [\"compete\", \"competition\", \"non-compete\", \"non-competition\", \"non compete\", \"non competition\" ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FtXkVJ9SdrWC"},"outputs":[],"source":["# Already existed words in Non-solicitation\n","\n","Nonsolicitation = [\"non-solicitation\", \"solicit\", \"non-solicit\", \"non-servicing\",   \"nonsolicitation\", \"nonsolicit\", \"nonservicing\",  \"solicit\",  \"non solicitation\", \"non solicit\", \"non servicing\",  \"no solicit\", \"no-solicit\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLH1jkxvdrWC"},"outputs":[],"source":["# Already existed words in Indemnification\n","\n","Indemnification = [\"indemnification\", \"indemnity\", \"hold-harmless\", \"hold harmless\", \"indemnify\", \"indemnified\", \"indemnifying\", \"defend\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xBmTGk9drWC"},"outputs":[],"source":["# Already existed words in Governing Law/ Jurisdiction\n","\n","Governing = [\"Texas\", \"Italy\", \"Italian\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GxJbRirmdrWD"},"outputs":[],"source":["# Already existed words in Exceptions\n","\n","Exceptions =  [\"was communicated by Disclosing Party to an unaffiliated third party free of any obligation of confidence\",   \"was communicated by Disclosing Party to a third party free of any obligation of confidence\", \"was communicated by Disclosing Party to  a third party without an obligation of confidence\", \"was disclosed by Disclosing Party to  a third party without an obligation of confidence\", \" was disclosed by Discloser to a third party without an obligation of confidentiality\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q1MWrImHdrWD"},"outputs":[],"source":["# Already existed words for Representatives\n","\n","# Different from the above, this part is for existence, if exist, yes, if not exist, output a warning sign about \"Missing Representatives\"\n","\n","Representatives = [\"responsible\", \"liable\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XGcJCMj5drWD"},"outputs":[],"source":["# Already existed words for Remedies\n","\n","# Same as the last one, this part is for existence, if exist, yes, if not exist, output a warning sign about \"Missing Remedies\"\n","Remedies = [\"injunction\", \"injunctive\",\"equitable\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vyLr__UudrWD"},"outputs":[],"source":["# Already existed words for Privacy / Personal Information\n","Privacy = [\"Personal Data\", \"Personal Information\",\"PII\", \"GDPR\", \"CCPA\", \"CPRA\", \"Privacy\"]"]},{"cell_type":"markdown","metadata":{"id":"hdAJFuGOdrWE"},"source":["Import docx and then analyzing it <br>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["93a50beef5a946629f22f73615c5a5ce","f73e4a68ff9c4b1cbf6461d45ea68556","4027606114c14a7fa20f618eccd695b4","53dc1fdf0ee5406e9513325928f09c56"]},"id":"Gmb_vTxldrWE","outputId":"a152f879-3862-41a8-c531-80453b58585e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93a50beef5a946629f22f73615c5a5ce","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f73e4a68ff9c4b1cbf6461d45ea68556","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4027606114c14a7fa20f618eccd695b4","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53dc1fdf0ee5406e9513325928f09c56","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 12s 12s/step\n"]}],"source":["# Test 3\n","\n","##           ###           ##  EEEEEEEEEEEEEEEE      III\n"," ##         ## ##         ##   E                     III\n","  ##       ##   ##       ##    E                     III\n","   ##     ##     ##     ##     EEEEEEEEEEEEEEE       III\n","    ##   ##       ##   ##      E                     III\n","     ## ##         ## ##       E                     III\n","      ##            ##         EEEEEEEEEEEEEEEE      III\n","\n","def ssp():\n","    \n","    # Recreate the exact same model, including its weights and the optimizer\n","    bert_model = tensorflow.keras.models.load_model('/content/drive/MyDrive/Final_pipeline/my_model.h5')\n","\n","    # # Show the model architecture\n","    # bert_model.summary()\n","    import spacy\n","    import re\n","    import nltk\n","    #en_core_web_sm: this package must be the newest version, currently 3.5.0   (by 2023.04.02)\n","    !pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz --quiet\n","    import en_core_web_sm\n","    #open text file in read mode\n","    docxFilename = \"/content/drive/MyDrive/Final_pipeline/NDA.docx\"\n","    output = pypandoc.convert_file([docxFilename], 'plain', outputfile=\"/content/drive/MyDrive/Final_pipeline/text_file.txt\")\n","    text_file = open(\"/content/drive/MyDrive/Final_pipeline/text_file.txt\", \"r\")\n","    \n","    #read whole file to a string\n","    data = text_file.read()\n","\n","    #close file\n","    text_file.close()\n","    #def a function which can split the doc into sentences\n","    def sentence_split(data):\n","        # nlp = spacy.load(\"en_core_web_sm\")\n","        nlp = en_core_web_sm.load()\n","        \n","        #take unicode string  \n","        sentences = nlp(data)\n","\n","\n","        sentencess = list(sentences.sents)\n","\n","        separated_sentences = []\n","\n","        for i in range(len(sentencess)):\n","            sentence = sentencess[i].text\n","            #sentence_separation = re.split('\\t|\\n|\\n\\t', sentence)\n","            sentence = sentence.replace('\\n', '')\n","            sentence = sentence.replace('\\t', '')\n","            sentence = sentence.replace('\\ufeff', '')\n","            sentence = sentence.replace('  ', '')\n","            sentence = sentence.replace('   ', '') #there are always multiple spaces, change them to single space universally\n","            sentences = re.split(r'\\.(?=\\s|$)', sentence)\n","            separated_sentences += sentences\n","\n","\n","        final_separated_sentences = [x for x in separated_sentences if x != '']\n","        return final_separated_sentences\n","\n","\n","    splitted = sentence_split(data)\n","\n","    #now define a function seperating the document into list of paragraphs\n","\n","    def par(splitted):\n","        ind = 0\n","        paragraphs = []\n","        splitting_index = []\n","\n","        #get the index of splitting\n","        for i in range(len(splitted)):\n","            if len(splitted[i].split()) < 5:\n","                splitting_index.append(i)\n","        \n","        for j in range(len(splitting_index)):\n","            temp_para = \"\"\n","            while ind < splitting_index[j]:\n","                temp_para += splitted[ind]\n","                ind += 1\n","            paragraphs.append(temp_para)\n","        return [p for p in paragraphs if len(p.split()) > 5]\n","\n","    paragraphs = par(splitted)\n","    #paragraphs\n","\n","    # Name of the BERT model to use\n","    model_name = 'bert-base-uncased'\n","\n","    # Max length of tokens\n","    max_length = 100\n","\n","    # Load transformers config and set output_hidden_states to False\n","    config = BertConfig.from_pretrained(model_name)\n","    config.output_hidden_states = False\n","\n","    # Load BERT tokenizer\n","    tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n","\n","\n","    test_x = tokenizer(\n","        text=par(splitted),\n","        add_special_tokens=True,\n","        max_length=max_length,\n","        truncation=True,\n","        padding=True, \n","        return_tensors='tf',\n","        return_token_type_ids = False,\n","        return_attention_mask = False,\n","        verbose = True)\n","\n","    #test_x\n","    #predict the possible value \n","    bert_result = bert_model.predict(test_x['input_ids'])\n","    #define a function, which can return the top 3 potential categories of a paragraph\n","    #this method would return a dictionary: key is each paragraph, the corresponding values a the top 3 potential categories\n","    target_names = ['DEF', 'EXP', 'GOV', 'REM', 'RIG', 'TER', 'WAR']\n","\n","    def par_cat(bert_result, paragraphs, target_names, topN):\n","        result = {}\n","        for i in range(len(bert_result[\"CAT\"])):\n","            val = np.argsort(bert_result[\"CAT\"][i])[::-1][:topN]\n","            result[paragraphs[i]] = [target_names[v] for v in val]\n","        return result\n","\n","    return par_cat(bert_result, paragraphs, target_names, 3)\n","\n","\n","original_input_dictionary = ssp()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8Y-q6duB8ZXD"},"outputs":[],"source":["## original_input_dictionary\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zpLyUbhvdrWF"},"outputs":[],"source":["paragraphs = list(original_input_dictionary.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"l6VO5tjYdrWF"},"outputs":[],"source":["# Segment paragraph into sentences. \n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","sentences_in_paragraphs = []  \n","\n","# take unicode string  \n","for i in range(len(paragraphs)):\n","    sentences_in_paragraph = nlp(paragraphs[i])\n","\n","    sentencess = list(sentences_in_paragraph.sents)\n","\n","    separated_sentences = []\n","    # list all sentences\n","    for j in range(len(sentencess)):\n","        sentence = sentencess[j].text\n","    \n","    \n","        sentence = sentence.replace('\\n\\n', ' ')\n","        sentences = re.split(r'\\.(?=\\s|$)', sentence)\n","\n","\n","        separated_sentences += sentences\n","\n","    final_separated_sentences = [x for x in separated_sentences if x != '']\n","\n","    sentences_in_paragraphs.append(final_separated_sentences)\n","\n","## sentences_in_paragraphs"]},{"cell_type":"markdown","metadata":{"id":"Fv5bIX9hdrWF"},"source":["Create dictionary for dealbreaker catergories and words with each category"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KGj38EuldrWG"},"outputs":[],"source":["# create a dictionary of sensitive words, grouped them by different categories. \n","\n","dealbreaker_words = {\n","    \"Parties\":Parties,\n","     \"Residuals/Memories\": Residuals,\n","     \"Limitation of Liability\": Limitation,\n","     \"Non-competition\": Noncompetition,\n","     \"Non-solicitation\":Nonsolicitation,\n","     \"Indemnification\": Indemnification,\n","     \"Governing Law/Jurisdiction\":Governing,\n","     \"Exceptions\":Exceptions,\n","     \"Privacy\": Privacy\n","}\n","\n","# The reason why not including remedies and representatives is due to the reason that\n","# their search methods are for existence (recall if they exist, then everything is ok, but if not exist, Warning)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"m7RKtuEldrWG"},"outputs":[],"source":["update_dealbreaker_words = {\n","    \"DEF\": Parties ,\n","    \"RIG\": Limitation + Limitation,\n","    \"EXP\": Exceptions + Limitation,\n","    \"WAR\": Parties + Residuals + Limitation + Noncompetition + Nonsolicitation + Indemnification + Governing + Exceptions + Privacy,\n","    \"GOV\": Governing,\n","    \"REM\": \"1\",\n","    \"TER\": Parties + Residuals + Limitation + Noncompetition + Nonsolicitation + Indemnification + Governing + Exceptions + Privacy\n","}"]},{"cell_type":"markdown","metadata":{"id":"OaqGk-r9drWG"},"source":["Find each paragraph's dealbreaker words"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PD1f7qJDdrWG"},"outputs":[],"source":["def find_specific_dealbreakers(input_keys, input_dictionary):\n","    my_dict = {\"new key\":[]}\n","    for i in range(len(list(input_dictionary.keys()))):\n","        for j in range(len(input_keys)):\n","            if input_keys[j] in list(input_dictionary.keys())[i]: \n","                my_dict[\"new key\"] += input_dictionary[list(input_dictionary.keys())[i]]\n","    return my_dict"]},{"cell_type":"markdown","metadata":{"id":"1q40_pzRdrWG"},"source":["First about dealbreaker words"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"J_h1HJgydrWG"},"outputs":[],"source":["combined_check_list = []\n","\n","for j in range(len(paragraphs)):\n","    #check_list = []\n","\n","    new_dict = find_specific_dealbreakers(list(original_input_dictionary.values())[j], update_dealbreaker_words)\n","\n","    for i in range(len(sentences_in_paragraphs[j])):\n","        sentence = sentences_in_paragraphs[j][i]\n","        # Iterate through the sensitive words grouped by category\n","        for category, words in new_dict.items():\n","            # Use regular expressions to search for the words in the sentence\n","            for word in words:\n","                matches = re.finditer(r'\\b'+word+r'\\b', sentence, re.IGNORECASE)\n","                for match in matches:\n","                    combined_check_list.append(match.group())\n","\n","\n","## combined_check_list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0Hk88djBdrWG","outputId":"511cbf7d-af77-48fb-e783-eb839dcd75bc"},"outputs":[{"data":{"text/plain":["['indirect']"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Clean it for unique words, Upper or lower case matters!\n","\n","combined_check_list = list(set(combined_check_list))\n","combined_check_list"]},{"cell_type":"markdown","metadata":{"id":"sdG-bG6IdrWH"},"source":["Then, about representative and rememdies. <br>\n","Here we need to find out whether they exist or not."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HzGTCbQrdrWH"},"outputs":[],"source":["# Representatives\n","representative_warning = True\n","\n","for j in range(len(original_input_dictionary)):\n","\n","    for i in range(len(sentences_in_paragraphs[j])):\n","        sentence = sentences_in_paragraphs[j][i]\n","        \n","        for word in Representatives:\n","            \n","            if word in sentence:\n","                representative_warning = False\n","                ## print(f\"Found '{word}' in sentence: {sentence}\")\n","                break\n","        \n","        else:\n","            continue\n","\n","        break\n","\n","\n","##representative_warning"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9HRJliUJdrWH"},"outputs":[],"source":["# Remedies\n","remedies_warning = True\n","\n","for j in range(len(paragraphs)):\n","\n","    for i in range(len(sentences_in_paragraphs[j])):\n","        sentence = sentences_in_paragraphs[j][i]\n","        \n","        for word in Remedies:\n","            if word in sentence:\n","                remedies_warning = False\n","                ## print(f\"Found '{word}' in sentence: {sentence}\")\n","                break\n","        else:\n","            continue\n","\n","        break\n","    "]},{"cell_type":"markdown","metadata":{"id":"TheoIVEXdrWH"},"source":["Export Stage"]},{"cell_type":"markdown","metadata":{"id":"iboSJ9fAdrWH"},"source":["Trial"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WSNdqdX_drWH"},"outputs":[],"source":["##           ###           ##  EEEEEEEEEEEEEEEE      III\n"," ##         ## ##         ##   E                     III\n","  ##       ##   ##       ##    E                     III\n","   ##     ##     ##     ##     EEEEEEEEEEEEEEE       III\n","    ##   ##       ##   ##      E                     III\n","     ## ##         ## ##       E                     III\n","      ##            ##         EEEEEEEEEEEEEEEE      III"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WVBG9o9wdrWH"},"outputs":[],"source":["# Import document again!\n","# change the path to your current path\n","doc=docx.Document('/content/drive/MyDrive/Final_pipeline/NDA.docx')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TVAIufNcdrWH"},"outputs":[],"source":["def find_key_by_value(value, dictionary):\n","    keys = []\n","    for key, val in dictionary.items():\n","        if isinstance(val, list):\n","            if any(v.lower() == value.lower() for v in val):\n","                keys.append(key)\n","        elif val.lower() == value.lower():\n","            keys.append(key)\n","    return keys if keys else None  # if value is not found in any key"]},{"cell_type":"markdown","metadata":{"id":"PeYez2YCdrWH"},"source":["Giving comment on dealbreaker words"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"haDg05zodrWI"},"outputs":[],"source":["def split_text(text, word):\n","    pattern = re.compile(r'([\\S\\s]*)(\\b{})([\\S\\s]*)'.format(word))\n","    match = pattern.search(text)\n","    if match:\n","        return match.groups()\n","    return None\n","\n","def split_Runs(doc,word):\n","    for p in doc.paragraphs:\n","        if p.text.find(word) != -1:\n","            virtualRuns=p.runs\n","            p.text = \"\"\n","            for r in virtualRuns:\n","                if r.text.find(word) != -1:\n","                    before,word,after = split_text(r.text, word)\n","                    p.add_run(before)\n","                    p.add_run()\n","                    p.add_run(word)\n","                    p.add_run(after)\n","                else:\n","                    p.add_run(r.text)\n","    return doc\n","\n","def style_Token2(doc,word, dictionary1, comment=True):\n","    for p in doc.paragraphs:\n","        for i,r in enumerate(p.runs):\n","          if (p.text.find(word + \",\")) != -1 or (p.text.find(word + \".\"))  != -1 or (p.text.find(word + \" \")) != -1:\n","            if p.runs[i].text.find(word) != -1:\n","                p.runs[i].font.highlight_color = WD_COLOR_INDEX.RED\n","                if comment:\n","                    corresponded_key = find_key_by_value(word,dictionary1)\n","                    p.runs[i-1].add_comment(f'Wanring: \\\"{word}\\\" is classified as a category {corresponded_key} dealbreaker word.\\nStrongly Advice its Immediate Removal',author=\"DG1\")\n","\n","\n","    return doc\n","\n","\n","\n","# combined_check_list\n","\n","for keyword in combined_check_list:\n","    doc=split_Runs(doc,keyword)    \n","\n","for keyword in combined_check_list:\n","    doc=style_Token2(doc,keyword, dealbreaker_words,True)"]},{"cell_type":"markdown","metadata":{"id":"jd7fcIV0drWI"},"source":["Giving Comment on Representative and Remedies <br>\n","if they exist, nothing will happen <br>\n","if they do not exist, output a warning comment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ABnYHGOYdrWI"},"outputs":[],"source":["def check_existence_for_remedy_representative(doc,warning_1 = True, warning_2 = True):\n","    if warning_1:\n","        paragraph1 = doc.add_paragraph()\n","        comment1 = paragraph1.add_comment(\"Warning: This document is lack of Representative Category information\", author = 'DG1')\n","    if warning_2:\n","        paragraph2 = doc.add_paragraph()\n","        comment2 = paragraph2.add_comment(\"Warning: This document is lack of Remedy Category information\",author = 'DG1')\n","\n","\n","    return doc\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zcWyCYY2drWI"},"outputs":[],"source":["doc = check_existence_for_remedy_representative(doc,representative_warning,remedies_warning)"]},{"cell_type":"markdown","metadata":{"id":"OPnJkZj2drWI"},"source":["Output path"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gnt1-MJcdrWI"},"outputs":[],"source":["##           ###           ##  EEEEEEEEEEEEEEEE      III\n"," ##         ## ##         ##   E                     III\n","  ##       ##   ##       ##    E                     III\n","   ##     ##     ##     ##     EEEEEEEEEEEEEEE       III\n","    ##   ##       ##   ##      E                     III\n","     ## ##         ## ##       E                     III\n","      ##            ##         EEEEEEEEEEEEEEEE      III"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zi3y-o7ddrWI"},"outputs":[],"source":["# change the path to your output path\n","doc.save('/content/drive/MyDrive/Final_pipeline/NDA_db_result.docx')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lKT5pvwydrWI"},"outputs":[],"source":["end_time = time.time()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gcPQ_DckdrWI"},"outputs":[],"source":["total_time = end_time - start_time"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"B4AXvsFQdrWJ","outputId":"0c1f9762-757b-4951-d611-efc501100fa2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total time taken: 95.52 seconds\n"]}],"source":["print(f\"Total time taken: {total_time:.2f} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MLjsNP_6drWJ"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}